import os

from trainer import Trainer, TrainerArgs

from TTS.config.shared_configs import BaseDatasetConfig
from TTS.tts.datasets import load_tts_samples
from TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig
from TTS.utils.manage import ModelManager
import wandb
import random

wandb.init(
    project="persian_tts",  # Updated project name
    name="Arman_21FEB",  # Run name, can be adjusted as needed
    sync_tensorboard=True,  # Enable TensorBoard syncing
    entity="bahareh-arghavani"  # Add your wandb username or team name here
)


# Logging parameters
RUN_NAME = "GPT_XTTS_v2.0_Arman_fa"
PROJECT_NAME = "XTTS_trainer"
DASHBOARD_LOGGER = "tensorboard"
LOGGER_URI = None
# start a new wandb run to track this script

# Set here the path that the checkpoints will be saved. Default: ./run/training/
OUT_PATH = os.path.join(os.path.dirname(os.path.abspath(__file__)), "fa_run", "training")

# Training Parameters
OPTIMIZER_WD_ONLY_ON_WEIGHTS = True  # for multi-gpu training please make it False
START_WITH_EVAL = True  # if True it will star with evaluation
BATCH_SIZE = 3  # set here the batch size
GRAD_ACUMM_STEPS = 84  # set here the grad accumulation steps
# Note: we recommend that BATCH_SIZE * GRAD_ACUMM_STEPS need to be at least 252 for more efficient training. You can increase/decrease BATCH_SIZE but then set GRAD_ACUMM_STEPS accordingly.

# Define here the dataset that you want to use for the fine-tuning on.
config_dataset = BaseDatasetConfig(
    formatter="mozilla",
    dataset_name="Arman_male",
    path="/data/notebook_files/datasets/Armna_5s",
    meta_file_train="/data/notebook_files/datasets/Armna_5s/metadata.csv",
    language="fa",
)

# Add here the configs of the datasets
DATASETS_CONFIG_LIST = [config_dataset]

# Define the path where XTTS v2.0.1 files will be downloaded
CHECKPOINTS_OUT_PATH = os.path.join(OUT_PATH, "XTTS_v2.0_original_model_files/")
os.makedirs(CHECKPOINTS_OUT_PATH, exist_ok=True)


# DVAE files
DVAE_CHECKPOINT_LINK = "https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth"
MEL_NORM_LINK = "https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth"

# Set the path to the downloaded files
DVAE_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(DVAE_CHECKPOINT_LINK))
MEL_NORM_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(MEL_NORM_LINK))

# download DVAE files if needed
if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):
    print(" > Downloading DVAE files!")
    ModelManager._download_model_files([MEL_NORM_LINK, DVAE_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True)


# Download XTTS v2.0 checkpoint if needed
TOKENIZER_FILE_LINK = "https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json"
XTTS_CHECKPOINT_LINK = "https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth"

# XTTS transfer learning parameters: You we need to provide the paths of XTTS model checkpoint that you want to do the fine tuning.
TOKENIZER_FILE = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(TOKENIZER_FILE_LINK))  # vocab.json file
XTTS_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, os.path.basename(XTTS_CHECKPOINT_LINK))  # model.pth file

# download XTTS v2.0 files if needed
if not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):
    print(" > Downloading XTTS v2.0 files!")
    ModelManager._download_model_files(
        [TOKENIZER_FILE_LINK, XTTS_CHECKPOINT_LINK], CHECKPOINTS_OUT_PATH, progress_bar=True
    )


# Training sentences generations
SPEAKER_REFERENCE = [
    "/data/notebook_files/datasets/test/wavs/Motaba_shakori_clip11.wav"  # speaker reference to be used in training test sentences
]
LANGUAGE = config_dataset.language


def main():
    # init args and config
    model_args = GPTArgs(
        max_conditioning_length=132300,  # 6 secs
        min_conditioning_length=66150,  # 3 secs
        debug_loading_failures=False,
        max_wav_length=255995,  # ~11.6 seconds
        max_text_length=200,
        mel_norm_file=MEL_NORM_FILE,
        dvae_checkpoint=DVAE_CHECKPOINT,
        xtts_checkpoint=XTTS_CHECKPOINT,  # checkpoint path of the model that you want to fine-tune
        tokenizer_file=TOKENIZER_FILE,
        gpt_num_audio_tokens=1026,
        gpt_start_audio_token=1024,
        gpt_stop_audio_token=1025,
        gpt_use_masking_gt_prompt_approach=True,
        gpt_use_perceiver_resampler=True,
    )
    # define audio config
    audio_config = XttsAudioConfig(sample_rate=22050, dvae_sample_rate=22050, output_sample_rate=24000)
    # training parameters config
    config = GPTTrainerConfig(
        output_path=OUT_PATH,
        model_args=model_args,
        run_name=RUN_NAME,
        project_name=PROJECT_NAME,
        run_description="""
            GPT XTTS training
            """,
        dashboard_logger=DASHBOARD_LOGGER,
        logger_uri=LOGGER_URI,
        audio=audio_config,
        batch_size=BATCH_SIZE,
        batch_group_size=48,
        eval_batch_size=BATCH_SIZE,
        num_loader_workers=8,
        eval_split_max_size=256,
        print_step=50,
        plot_step=100,
        log_model_step=1000,
        save_step=10000,
        save_n_checkpoints=1,
        save_checkpoints=True,
        # target_loss="loss",
        print_eval=False,
        # Optimizer values like tortoise, pytorch implementation with modifications to not apply WD to non-weight parameters.
        optimizer="AdamW",
        optimizer_wd_only_on_weights=OPTIMIZER_WD_ONLY_ON_WEIGHTS,
        optimizer_params={"betas": [0.9, 0.96], "eps": 1e-8, "weight_decay": 1e-2},
        lr=5e-06,  # learning rate
        lr_scheduler="MultiStepLR",
        # it was adjusted accordly for the new step scheme
        lr_scheduler_params={"milestones": [50000 * 18, 150000 * 18, 300000 * 18], "gamma": 0.5, "last_epoch": -1},
        test_sentences=[
            {
                "text": "Ø²ÛŒÙ† Ù‡Ù…Ø±Ù‡Ø§Ù† Ø³Ø³Øª Ø¹Ù†Ø§ØµØ±ØŒ Ø¯Ù„Ù… Ú¯Ø±ÙØª.",
                "speaker_wav": SPEAKER_REFERENCE,
                "language": LANGUAGE,
            },
            {
                "text": "Ø¢Ù†Ú©Ù‡ Ù…Ø¯Ø§Ù… Ø¨Ù‡ Ú©Ø§Ø± Ø¯ÛŒÚ¯Ø±Ø§Ù† Ø³Ø±Ú© Ù…ÛŒ Ú©Ø´Ø¯ Ùˆ Ú©Ù†Ø¬Ú©Ø§Ùˆ Ø§Ø³Øª ØªØ§ Ø¨Ø¨ÛŒÙ†Ø¯ Ø¢Ù†Ù‡Ø§ Ú†Ù‡ Ù…ÛŒ Ú©Ù†Ù†Ø¯ Ù…Ø§Ù†Ù†Ø¯ Ø³Ø§ÛŒÙ‡ Ø§ÛŒÛŒ Ø¨Ø± Ø¯ÛŒÙˆØ§Ø± Ø§Ø³Øª Ú©Ù‡ Ù…Ø¯Ø§Ù… Ø¨Ø¯Ù†Ø¨Ø§Ù„ Ù…Ø§ Ù…ÛŒ Ø¯ÙˆØ¯ Ø¨Ø¯ÙˆÙ† Ø¢Ù†Ú©Ù‡ Ø§Ø² Ø®ÙˆØ¯ Ø§Ø®ØªÛŒØ§Ø±ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯",
                "speaker_wav": SPEAKER_REFERENCE,
                "language":LANGUAGE,
            },
            {
                "text": "Ø¨ÛŒØ§ ØªØ§ Ú¯Ù„ Ø¨Ø±Ø§ÙØ´Ø§Ù†ÛŒÙ… Ùˆ Ù…ÛŒ Ø¯Ø± Ø³Ø§ØºØ± Ø§Ù†Ø¯Ø§Ø²ÛŒÙ….",
                "speaker_wav":SPEAKER_REFERENCE,
                "language": LANGUAGE,
            },
            {
                "text": "Ø±Ø§Ù‡ Ø¢Ø´ØªÛŒ Ø±Ø§ Ú©Ø³ÛŒ Ø¨Ø§ÛŒØ¯ Ø¨ÛŒØ§Ø¨Ø¯ Ú©Ù‡ Ø®ÙˆØ¯ Ø³Ø¨Ø¨ Ø¬Ø¯Ø§ÛŒÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª",
                "speaker_wav": SPEAKER_REFERENCE,
                "language": LANGUAGE,
            },
            {
                "text": "Ø¨Ù†ÛŒ Ø¢Ø¯Ù… Ø§Ø¹Ø¶Ø§ÛŒ ÛŒÚ© Ù¾ÛŒÚ©Ø±Ù†Ø¯, Ú©Ù‡ Ø¯Ø± Ø¢ÙØ±ÛŒÙ†Ø´ Ø² ÛŒÚ© Ú¯ÙˆÙ‡Ø±Ù†Ø¯.",
                "speaker_wav": SPEAKER_REFERENCE,
                "language": LANGUAGE,
            },
            {
                "text": "Ø³Ù‡Ø§Ù… Ø²Ù†Ø¯Ú¯ÛŒ Ø¨Ù‡ 10 Ø¯Ø±ØµØ¯ Ùˆ Ø³Ù‡Ø§Ù… Ø¨ÛŒØªÚ©ÙˆÛŒÙ† Ú¯ÙˆÚ¯Ù„ Ø¨Ù‡ 33 Ø¯Ø±ØµØ¯ Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØª.",
                "speaker_wav": SPEAKER_REFERENCE,
                "language": LANGUAGE,
            },
            {
                "text": "ÛŒÙ‡ Ø¯Ùˆ Ø¯Ù‚Ù‡ Ù‡Ù… Ø¨Ù‡ Ø­Ø±ÙÙ… Ú¯ÙˆØ´ Ú©Ù†ØŒ Ù†Ú¯Ùˆ Ù†Ú¯ÙˆØ´ÛŒØ¯Ù… Ùˆ Ù†Ø­Ø±ÙÛŒØ¯ÛŒ.",
                "speaker_wav": SPEAKER_REFERENCE,
                "language": LANGUAGE,
            },
            {
                "text": "Ù…Ø·Ø§Ù„Ø¹Ø§Øª Ù†Ø´Ø§Ù† Ø¯Ø§Ø¯Ù‡â€ŒØ§Ù†Ø¯ Ú©Ù‡ 40 Ø¯Ø±ØµØ¯ Ø§Ø² Ø¯Ø§Ù†Ø´â€ŒØ¢Ù…ÙˆØ²Ø§Ù†ØŒ Ø¯Ø± Ø³Ø§Ù„ ØªØ­ØµÛŒÙ„ÛŒ 2024-2023ØŒ Ø¯Ø± Ø¯ÙˆØ±Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù†Ù„Ø§ÛŒÙ† Ø´Ø±Ú©Øª Ú©Ø±Ø¯Ù‡â€ŒØ§Ù†Ø¯ØŒ Ú©Ù‡ Ø§ÛŒÙ† Ø§Ù…Ø± Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ ØªØºÛŒÛŒØ± Ú†Ø´Ù…Ú¯ÛŒØ±ÛŒ Ø¯Ø± Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø¯Ø± Ø§ÛŒØ±Ø§Ù† Ù…ÛŒâ€ŒØ¨Ø§Ø´Ø¯.",
                "speaker_wav": SPEAKER_REFERENCE,
                "language": LANGUAGE,
            },
            {
                "text": "Ù…Ù† Ø¨ÙˆØ¯Ù… Ùˆ Ø¢Ø¨Ø¬ÛŒ ÙÙˆØªÛŒÙ†Ø§ØŒ Ùˆ Ø­Ø§Ù„Ø§ Ø±Ù¾ØªÛŒ Ù¾ØªÛŒÙ†Ø§. Ø§ÛŒÙ† Ø´Ø¹Ø± ÛŒÚ©ÛŒ Ø§Ø² Ø§Ø´Ø¹Ø§Ø± Ù…Ø¹Ø±ÙˆÙ Ø±Ùˆ Ø­ÙˆØ¶ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± Ú©ÙˆÚ†Ù‡ Ø¨Ø§Ø²Ø§Ø± ØªÙ‡Ø±Ø§Ù† Ø²Ù…Ø²Ù…Ù‡ Ù…ÛŒ Ø´Ø¯Ù‡ Ø§Ø³Øª.",
                "speaker_wav": SPEAKER_REFERENCE,
                "language": LANGUAGE,
            },
            {
                "text": "Ø¯Ø± Ø¨ÙˆØ¯Ø¬Ù‡ Ø³Ø§Ù„ 2025ØŒ Ø¯ÙˆÙ„Øª Ø§ÛŒØ±Ø§Ù† Ø§Ø¹Ù„Ø§Ù… Ú©Ø±Ø¯ Ú©Ù‡ 15 Ø¯Ø±ØµØ¯ Ø§Ø² Ú©Ù„ Ø¨ÙˆØ¯Ø¬Ù‡ØŒ Ù…Ø¹Ø§Ø¯Ù„ ØªÙ‚Ø±ÛŒØ¨Ø§Ù‹ 30 Ù…ÛŒÙ„ÛŒØ§Ø±Ø¯ Ø¯Ù„Ø§Ø±ØŒ Ø¨Ù‡ ØªÙˆØ³Ø¹Ù‡ Ø²ÛŒØ±Ø³Ø§Ø®Øªâ€ŒÙ‡Ø§ÛŒ Ø­Ù…Ù„â€ŒÙˆÙ†Ù‚Ù„ Ùˆ 25 Ø¯Ø±ØµØ¯ Ø¯ÛŒÚ¯Ø±ØŒ Ø¨Ù‡ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¨Ø®Ø´ Ø¨Ù‡Ø¯Ø§Ø´Øª Ùˆ Ø¯Ø±Ù…Ø§Ù† Ø§Ø®ØªØµØ§Øµ ÛŒØ§ÙØªÙ‡ Ø§Ø³Øª.",
                "speaker_wav": SPEAKER_REFERENCE,
                "language": LANGUAGE,
            },
            {
                "text": "ØªØ­Ù‚ÛŒÙ‚Ø§Øª Ø¨Ø§Ø²Ø§Ø± Ù†Ø´Ø§Ù† Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ ÙØ±ÙˆØ´ Ú¯ÙˆØ´ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¯Ø± Ø§ÛŒØ±Ø§Ù†ØŒ Ø¯Ø± Ø³Ù‡ Ù…Ø§Ù‡Ù‡ Ø§ÙˆÙ„ Ø³Ø§Ù„ 2024ØŒ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù…Ø¯Øª Ù…Ø´Ø§Ø¨Ù‡ Ø¯Ø± Ø³Ø§Ù„ Ù‚Ø¨Ù„ØŒ 55 Ø¯Ø±ØµØ¯ Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØªÙ‡ Ú©Ù‡ Ø§ÛŒÙ† Ù¾Ø¯ÛŒØ¯Ù‡ Ø¨ÛŒØ´ØªØ± Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø¹Ø±Ø¶Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ùˆ Ú©Ø§Ù‡Ø´ Ù‚ÛŒÙ…Øªâ€ŒÙ‡Ø§ Ø§Ø³Øª.",
                "speaker_wav": SPEAKER_REFERENCE,
                "language": LANGUAGE,
            },
            {
                "text": "Ø³Ù„Ø§Ù…ØŒ Ø®ÙˆØ´Ø­Ø§Ù„Ù… Ú©Ù‡ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ù…ÛŒâ€ŒØ¨ÛŒÙ†Ù…ØªØ› Ø§Ù…ÛŒØ¯ÙˆØ§Ø±Ù… Ú©Ù‡ Ø§ÛŒÙ† Ù‡ÙØªÙ‡ Ø¨Ø±Ø§Øª Ø®ÙˆØ¨ Ú¯Ø°Ø´ØªÙ‡ Ø¨Ø§Ø´Ù‡.",
                "speaker_wav": SPEAKER_REFERENCE,
                "language": LANGUAGE,
            },
            {
                "text": "Ø¨Ø±Ø§ÛŒ Ø¬Ø´Ù† ØªÙˆÙ„Ø¯ Ù¾Ø¯Ø±Ù… Ù‚ØµØ¯ Ø¯Ø§Ø±Ù… ÛŒÚ© Ù…Ù‡Ù…Ø§Ù†ÛŒ Ú©ÙˆÚ†Ú© Ø¨Ø§ Ø®Ø§Ù†ÙˆØ§Ø¯Ù‡ Ùˆ Ø¯ÙˆØ³ØªØ§Ù† Ù†Ø²Ø¯ÛŒÚ© ØªØ±ØªÛŒØ¨ Ø¨Ø¯Ù‡Ù… Ø› Ø¯ÙˆØ³Øª Ø¯Ø§Ø±ÛŒ ØªÙˆ Ù‡Ù… Ø¨ÛŒØ§ÛŒØŸ",
                "speaker_wav": SPEAKER_REFERENCE,
                "language": LANGUAGE,
            },
            {
                "text": "Ø¯ÛŒØ´Ø¨ ÛŒÚ© ÙÛŒÙ„Ù… Ø¬Ø°Ø§Ø¨ Ø¯Ø± ØªÙ„ÙˆÛŒØ²ÛŒÙˆÙ† Ø¯ÛŒØ¯Ù… Ú©Ù‡ Ø¯Ø± Ù…ÙˆØ±Ø¯ ØªØ§Ø±ÛŒØ®Ú†Ù‡ ØªÙ…Ø¯Ù†â€ŒÙ‡Ø§ÛŒ Ø¨Ø§Ø³ØªØ§Ù† Ø¨ÙˆØ¯Ø› ÙˆØ§Ù‚Ø¹Ø§Ù‹ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙÛŒØ¯ Ùˆ Ø¬Ø§Ù„Ø¨ÛŒ Ø¯Ø§Ø´Øª.",
                "speaker_wav": SPEAKER_REFERENCE,
                "language": LANGUAGE,
            },

        ],
    )
    # init the model from config
    model = GPTTrainer.init_from_config(config)

    # load training samples
    train_samples, eval_samples = load_tts_samples(
        DATASETS_CONFIG_LIST,
        eval_split=True,
        eval_split_max_size=config.eval_split_max_size,
        eval_split_size=config.eval_split_size,
    )

    # init the trainer and ğŸš€
    trainer = Trainer(
        TrainerArgs(
            restore_path=None,  # xtts checkpoint is restored via xtts_checkpoint key so no need of restore it using Trainer restore_path parameter
            skip_train_epoch=False,
            start_with_eval=START_WITH_EVAL,
            grad_accum_steps=GRAD_ACUMM_STEPS,
        ),
        config,
        output_path=OUT_PATH,
        model=model,
        train_samples=train_samples,
        eval_samples=eval_samples,
    )
    trainer.fit()
    # [optional] finish the wandb run, necessary in notebooks
    wandb.finish()


if __name__ == "__main__":
    main()
